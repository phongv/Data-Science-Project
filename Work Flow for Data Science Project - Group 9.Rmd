---
title: "Work Flow Data Science Project"
date: "5/28/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Package

```{r Packages, message=FALSE}

library(tidyverse) # data manipulation
library(quanteda) # text pre-processing
library(quanteda.dictionaries)
library(tidytext) # text analysis
library(lubridate) # date function
library(gridExtra) # multiple graphs
library(stm) # topic modelling
library(stmCorrViz)
library(furrr)
library(tm)

```


# Flow 1 - Load + Clean data

## Reddit

```{r, message=FALSE}

reddit_data <- read_rds("Reddit_Data.rds")

```

## Twitter

```{r, message=FALSE}

tweets_data <- read_csv("twitter_coronavirus.csv")

# Transform the date column

tweets_data <- tweets_data %>% 
  separate(created_at, c("wday", "month", "day", "time", "plus", "year"), 
           sep = " ") %>% 
  mutate(date = paste(month, day, year, sep = " ")) %>% 
  select(c("text", "date"))

tweets_data$date <- as.Date(tweets_data$date, format = "%b %d %Y")
tweets_data <- tweets_data %>% 
  mutate(week = isoweek(date)) %>% 
  filter(week > 2)

# Clean hashtags, links and special characters

tweets_data$text <- tweets_data$text %>% 
  str_replace_all("#", " ") %>%
  str_remove_all("(?<=^|\\s)http[^\\s]+") %>% 
  str_remove_all("[^a-zA-Z0-9 ]") %>% 
  trimws()

# Remove blank text after cleaning

tweets_data <- tweets_data %>% 
  filter(str_count(text, pattern = boundary("word")) > 1)
```


## News

```{r, message=FALSE}

news_data <- read_csv("news_coronavirus.csv")

# Add week and remove unnecessary column

news_data <- news_data %>% 
  mutate(week = isoweek(publish_date)) %>% 
  select(-title)

```

## Data Distribution

The data distribution can be seen in Figure 1. The number of news reported on COVID-19 in the U.S were relatively low in the first 6 weeks (week 3 - week 8), and the subreddit CoronavirusUS was not even created until week 7. However, there was a dramatic increase starting from week 9 and peaked in week 11, 12 and 13 for Twitter, Reddit and News respectively. This was the end of March when the number of infected cases in the country surged.

```{r Data Count, echo=FALSE}

plot_news <- news_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "News")

plot_tweets <- tweets_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "Twitter")

plot_reddit <- reddit_data %>%
  group_by(week) %>%
  summarise(n = n()) %>% 
  ungroup() %>% 
  mutate(source = "Reddit")

```

```{r Data Distribution, echo=FALSE}

news_distribution <- plot_news %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 10) +
  labs(x = "week", y = "number of documents", subtitle = "News") +
  theme(
    axis.title = element_text(size = 14),
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    panel.border = element_rect("lightgray", fill = NA),
    plot.subtitle = element_text(hjust = 0.5, size = rel(1.8)))

tweets_distribution <- plot_tweets %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 10) +
  labs(x = "week", y = NULL, subtitle = "Twitter") +
  theme(
    axis.title = element_text(size = 14),
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    panel.border = element_rect("lightgray", fill = NA),
    plot.subtitle = element_text(hjust = 0.5, size = rel(1.8)))

reddit_distribution <- plot_reddit %>%
  ggplot(aes(x = week)) + 
  geom_col(aes(y = n, fill = -n), show.legend = FALSE) +
  scale_x_continuous(n.breaks = 6) +
  labs(x = "week", y = NULL, subtitle = "Reddit") +
  theme(
    axis.title = element_text(size = 14),
    axis.text.x = element_text(size = 14, hjust = .5, vjust = .5),
    axis.text.y = element_text(size = 14),
    panel.border = element_rect("lightgray", fill = NA),
    plot.subtitle = element_text(hjust = 0.5, size = rel(1.8)))

grid.arrange(news_distribution, tweets_distribution, reddit_distribution,
             ncol = 3)

```


# Flow 2 - Corpus - Token

For text data preparation, we focus on quanteda package in R, which is one of the most popular R packages for the quantitative analysis of textual data. Firstly, we created a corpus for the data set of each source. A corpus consists of a collection of documents (each document is an article, a Reddit comment or a tweet), and the document variables which describe the characteristics of the document, for instance published date and source. Subsequently, we tokenized each corpus, which separates the text into into its single words (also called terms or tokens). 

We pre-processed the data sets by lowering upper case letters and removing the numbers, punctuations, symbols, URLs and separators. The main idea is to reduce the final amount of terms extracted, which is important in order to improve the accuracy of both topic modeling and sentiment analysis. If two words are similar it is convenient to combine them as one unique word. Moreover, if a word is not relevant for the analysis, it can be removed from. Hence, we also implemented stopword removal and lemmatization technique. Stopwords are words that appear in texts but do not give the text a substantial meaning (e.g., "the", "a", or "for") and lemmatization deals with the inflected forms of words by replacing them with their base forms.

In the next step, we generated a document-feature matrix (also known as document-term matrix) for each source. It represents how frequently terms occur in the corpus by counting single terms. We kept only the top 5% of the most frequent features (minimum term frequency set at 0.95) that present in less than 10% of all documents (maximum document frequency set at 0.1) to focus on common but distinctive features.

## Reddit

### Create a corpus 

```{r}

corpus_Reddit_data <- corpus(reddit_data, text_field = c("comments"), unique_docnames = F) %>%
  `docnames<-`(reddit_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Reddit_data <- corpus_Reddit_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_remove(pattern = c("subreddit", "subredditmessagecomposetorcoronavirusus", "652000", "discussion", "thread", "subreddits", "people")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

```{r}

Reddit_DFM <- Token_Reddit_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Reddit_DFM <- Reddit_DFM[ntoken(Reddit_DFM) > 0,]
```


## Twitter

### Create a corpus 

```{r}

corpus_Twitter_data <- corpus(tweets_data, text_field = c("text"),
                             unique_docnames = F) %>%
  `docnames<-`(tweets_data$week)

```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")
```


### Create Token

```{r}

Token_Twitter_data <- corpus_Twitter_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE,
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = FALSE) %>%
  tokens_remove(pattern = c("people", "covid19", "covid", "rt", "dont")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

```


### Keep Top features

```{r}

Twitter_DFM <- Token_Twitter_data %>% 
  tokens_remove("") %>%
  dfm() %>%
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

Twitter_DFM <- Twitter_DFM[ntoken(Twitter_DFM) > 0,]
```


## News

### Create a corpus 

```{r}

corpus_News_data <- corpus(news_data, text_field = c("text"), 
                           unique_docnames = F) %>%
  `docnames<-`(news_data$week) # update the name of document in the corpus
#head(corpus_News_data, 5) # take a look at the corpus
```

### Stop Words & Base formed words

```{r}

stopwords_extended <- readLines("stopwords_en.txt", encoding = "UTF-8")

lemma_data <- read.csv("baseform_en.tsv", encoding = "UTF-8")

```

### Create Tokens

```{r}

Token_news_data <- corpus_News_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, 
         remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = FALSE) %>%
  tokens_remove(pattern = c("amid", "updates", "live", "video", 
                            "didn", "briefing")) %>%
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, 
                 valuetype = "glob") %>%
  tokens_replace(pattern = c("covid-19", "ncov", "cov"), 
                 c("coronavirus", "coronavirus", "coronavirus"),
                 valuetype = "fixed") %>% 
  tokens_remove(pattern = stopwords_extended, padding = F)

```


### Keep Top features

```{r}

News_DFM <- Token_news_data %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile",
           max_docfreq = 0.1, docfreq_type = "prop")

# We remove the token = 0

News_DFM <- News_DFM[ntoken(News_DFM) > 0, ]
#head(News_DFM, n = 5, nf = 8)

```


# Flow 3 - Co-occurrence analysis

By way of definition, co-occurrence networks are the collective interconnection of tokens (terms) based on their paired presence within a corpus. Networks are generated by connecting pairs of terms using significant level of co-occurrence. In general, co-occurrence aim is to find similarities in meaning between word pairs and/or similarities in meaning among/within word patterns (corpus). 
In a specific linguistic context, two (or more) words that have similar co-occurrence patterns tend to be positioned closer together in semantic space and tend to resemble each other in meaning. Co-occurrence statistic not only count joint occurrence it also to determine the significance of these joint. We only keep the most significant joint occurrence of our defined “target word” with any other “terms” in the corpus. We choose to use co-occurrence to explore the data because the co-occurrence provides a clear picture about the relationship between our “target word” with other “terms” in each corpus. 

First, we explored the data through the co-occurrence analysis, by looking into the co-occurrence network, we can see how the keyword “coronavirus” is mapped in each data source. To perform co-occurrence analysis, we need to standardize the target keyword by replacing all tokens that contain “Corona”, “Virus”, “Covid19”, “ncov” into coronavirus. The blue lines indicate the connections that directly related to our keyword.

The network model in figure 2 shows the joint occurrence of coronavirus keyword. The network was created using `visNetwork` which is a R package for network visualization, using `vis.js javascript` library. The package allows us to create an interactive network visualization similar to `D3` (we only show here the plot of Twitter data, News & Reddit plots can be found in the Appendix 2). The chart shows that, the word “coronavirus” is usually linked with terms like spread, case, infect in all three sources.

`Source: https://tm4ss.github.io/docs/Tutorial_5_Co-occurrence.html`

## Reddit Network

```{r}
Corona_Dictionary <- read_csv("Corona_Dictionary2.csv")


Token_reddit_network <- corpus_Reddit_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(Corona_Dictionary$keyword, Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T) %>%
  tokens_remove(pattern = c("subreddit", "subredditmessagecomposetorcoronavirusus", 
                            "652000", "discussion", "thread", "subreddits", "people",
                            "gt"))

# calculate multi-word unit candidates
Redit_collocations <- textstat_collocations(Token_reddit_network, min_count = 50)

Redit_collocations <- tokens_compound(Token_reddit_network, Redit_collocations)

minimumFrequency <- 10

Reddit_binDTM <- Redit_collocations %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")

Reddit_coocCounts <- t(Reddit_binDTM) %*% Reddit_binDTM

Reddit_coocTerm <- c("coronavirus")
Reddit_k <- nrow(Reddit_binDTM)
Reddit_ki <- sum(Reddit_binDTM[, Reddit_coocTerm])
Reddit_kj <- colSums(Reddit_binDTM)
names(Reddit_kj) <- colnames(Reddit_binDTM)
Reddit_kij <- Reddit_coocCounts[Reddit_coocTerm, ]

########## MI: log(Reddit_k*Reddit_kij / (Reddit_ki * Reddit_kj) ########
mutualInformationSig <- log(Reddit_k * Reddit_kij / (Reddit_ki * Reddit_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * Reddit_kij / (Reddit_ki + Reddit_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((Reddit_k * log(Reddit_k)) - (Reddit_ki * log(Reddit_ki)) - (Reddit_kj * log(Reddit_kj)) + (Reddit_kij * log(Reddit_kij)) 
               + (Reddit_k - Reddit_ki - Reddit_kj + Reddit_kij) * log(Reddit_k - Reddit_ki - Reddit_kj + Reddit_kij) 
               + (Reddit_ki - Reddit_kij) * log(Reddit_ki - Reddit_kij) + (Reddit_kj - Reddit_kij) * log(Reddit_kj - Reddit_kij) 
               - (Reddit_k - Reddit_ki) * log(Reddit_k - Reddit_ki) - (Reddit_k - Reddit_kj) * log(Reddit_k - Reddit_kj))


logsig <- logsig[order(logsig, decreasing=T)]

Reddit_resultOverView <- data.frame(
  names(sort(Reddit_kij, decreasing=T)[1:10]), sort(Reddit_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(Reddit_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")

# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
Reddit_coocTerm <- "coronavirus"

Reddit_coocs <- calculateCoocStatistics(Reddit_coocTerm, Reddit_binDTM, measure="LOGLIK")

# Reddit_resultGraph

Reddit_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# The structure of the temporary graph object is equal to that of the Reddit_resultGraph
Reddit_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
Reddit_tmpGraph[1:numberOfCoocs, 3] <- Reddit_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
Reddit_tmpGraph[, 1] <- Reddit_coocTerm
# Entry of the co-occurrences into the second column of the respective line
Reddit_tmpGraph[, 2] <- names(Reddit_coocs)[1:numberOfCoocs]
# Set the significances
Reddit_tmpGraph[, 3] <- Reddit_coocs[1:numberOfCoocs]

# Attach the triples to Reddit_resultGraph
Reddit_resultGraph <- rbind(Reddit_resultGraph, Reddit_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newReddit_coocTerm <- names(Reddit_coocs)[i]
  Reddit_coocs2 <- calculateCoocStatistics(newReddit_coocTerm, Reddit_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  Reddit_coocs2[1:10]
  
  # Structure of the temporary graph object
  Reddit_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  Reddit_tmpGraph[1:numberOfCoocs, 3] <- Reddit_coocs2[1:numberOfCoocs]
  Reddit_tmpGraph[, 1] <- newReddit_coocTerm
  Reddit_tmpGraph[, 2] <- names(Reddit_coocs2)[1:numberOfCoocs]
  Reddit_tmpGraph[, 3] <- Reddit_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  Reddit_resultGraph <- rbind(Reddit_resultGraph, Reddit_tmpGraph[2:length(Reddit_tmpGraph[, 1]), ])
}

require(igraph)

# Set the graph and type
Reddit_graphNetwork <- graph.data.frame(Reddit_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
Reddit_graphVs <- V(Reddit_graphNetwork)[degree(Reddit_graphNetwork) < 2]
# These edges are removed from the graph
Reddit_graphNetwork <- delete.vertices(Reddit_graphNetwork, Reddit_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(Reddit_graphNetwork)$color <- ifelse(V(Reddit_graphNetwork)$name == Reddit_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
Reddit_halfMaxSig <- max(E(Reddit_graphNetwork)$sig) * 0.5
E(Reddit_graphNetwork)$color <- ifelse(E(Reddit_graphNetwork)$sig > Reddit_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(Reddit_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(Reddit_graphNetwork)$size <- log(degree(Reddit_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(Reddit_graphNetwork)$size[V(Reddit_graphNetwork)$size < 5] <- 4

# edge thickness
E(Reddit_graphNetwork)$width <- 1

# Finaler Plot
Reddit_network <- toVisNetworkData(Reddit_graphNetwork)
visNetwork(nodes = Reddit_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = Reddit_network$edges %>% 
             mutate(color = NA), 
           main = "Corona keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE) %>%
  visSave(file = "Reddit_network.html")
```


## News Network

```{r}
New_Corona_Dictionary <- read_csv("News_Corona_Dictionary.csv") %>%
  add_row(keyword = "virus", replace = "coronavirus") %>%
  add_row(keyword = "covid-19", replace = "coronavirus")

Token_News_network <- corpus_News_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(New_Corona_Dictionary$keyword, New_Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

# calculate multi-word unit candidates
News_collocations <- textstat_collocations(Token_News_network, min_count = 50)

News_collocations <- tokens_compound(Token_News_network, News_collocations)

minimumFrequency <- 10

News_binDTM <- News_collocations %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")

# Matrix multiplication for cooccurrence counts

News_coocCounts <- t(News_binDTM) %*% News_binDTM

News_coocTerm <- c("coronavirus")
News_k <- nrow(News_binDTM)
News_ki <- sum(News_binDTM[, News_coocTerm])
News_kj <- colSums(News_binDTM)
names(News_kj) <- colnames(News_binDTM)
News_kij <- News_coocCounts[News_coocTerm, ]


########## MI: log(News_k*News_kij / (News_ki * News_kj) ########
mutualInformationSig <- log(News_k * News_kij / (News_ki * News_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * News_kij / (News_ki + News_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((News_k * log(News_k)) - (News_ki * log(News_ki)) - (News_kj * log(News_kj)) + (News_kij * log(News_kij)) 
               + (News_k - News_ki - News_kj + News_kij) * log(News_k - News_ki - News_kj + News_kij) 
               + (News_ki - News_kij) * log(News_ki - News_kij) + (News_kj - News_kij) * log(News_kj - News_kij) 
               - (News_k - News_ki) * log(News_k - News_ki) - (News_k - News_kj) * log(News_k - News_kj))


logsig <- logsig[order(logsig, decreasing=T)]

# Put all significance statistics in one Data-Frame
News_resultOverView <- data.frame(
  names(sort(News_kij, decreasing=T)[1:10]), sort(News_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(News_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")

# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
News_coocTerm <- "coronavirus"

News_coocs <- calculateCoocStatistics(News_coocTerm, News_binDTM, measure="LOGLIK")

News_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# The structure of the temporary graph object is equal to that of the News_resultGraph
News_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
News_tmpGraph[1:numberOfCoocs, 3] <- News_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
News_tmpGraph[, 1] <- News_coocTerm
# Entry of the co-occurrences into the second column of the respective line
News_tmpGraph[, 2] <- names(News_coocs)[1:numberOfCoocs]
# Set the significances
News_tmpGraph[, 3] <- News_coocs[1:numberOfCoocs]

# Attach the triples to News_resultGraph
News_resultGraph <- rbind(News_resultGraph, News_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newNews_coocTerm <- names(News_coocs)[i]
  News_coocs2 <- calculateCoocStatistics(newNews_coocTerm, News_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  News_coocs2[1:10]
  
  # Structure of the temporary graph object
  News_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  News_tmpGraph[1:numberOfCoocs, 3] <- News_coocs2[1:numberOfCoocs]
  News_tmpGraph[, 1] <- newNews_coocTerm
  News_tmpGraph[, 2] <- names(News_coocs2)[1:numberOfCoocs]
  News_tmpGraph[, 3] <- News_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  News_resultGraph <- rbind(News_resultGraph, News_tmpGraph[2:length(News_tmpGraph[, 1]), ])
}

require(igraph)

# Set the graph and type
News_graphNetwork <- graph.data.frame(News_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
News_graphVs <- V(News_graphNetwork)[degree(News_graphNetwork) < 2]
# These edges are removed from the graph
News_graphNetwork <- delete.vertices(News_graphNetwork, News_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(News_graphNetwork)$color <- ifelse(V(News_graphNetwork)$name == News_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
News_halfMaxSig <- max(E(News_graphNetwork)$sig) * 0.5
E(News_graphNetwork)$color <- ifelse(E(News_graphNetwork)$sig > News_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(News_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(News_graphNetwork)$size <- log(degree(News_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(News_graphNetwork)$size[V(News_graphNetwork)$size < 5] <- 4

# edge thickness
E(News_graphNetwork)$width <- 1

# Finaler Plot
library(visNetwork)
News_network <- toVisNetworkData(News_graphNetwork)
visNetwork(nodes = News_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = News_network$edges %>% 
             mutate(color = NA), 
           
           main = "Corona keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE) %>%
  visSave(file = "News_network.html")

```


## Twitter Network

```{r}
Twitter_Corona_Dictionary <- read_csv("Twitter_Corona_Dictionary.csv")


Token_Twitter_network <- corpus_Twitter_data %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE, remove_url = TRUE, remove_separators = TRUE) %>% 
  tokens_tolower(keep_acronyms = TRUE) %>%
  tokens_replace(Twitter_Corona_Dictionary$keyword, Twitter_Corona_Dictionary$replace) %>% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = "glob") %>% 
  tokens_remove(pattern = stopwords_extended, padding = T)

# calculate multi-word unit candidates
Redit_collocations <- textstat_collocations(Token_Twitter_network, min_count = 50)

Redit_collocations <- tokens_compound(Token_Twitter_network, Redit_collocations)

minimumFrequency <- 10

Twitter_binDTM <- Token_Twitter_network %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")

# Matrix multiplication for cooccurrence counts

Twitter_coocCounts <- t(Twitter_binDTM) %*% Twitter_binDTM

Twitter_coocTerm <- c("coronavirus")
Twitter_k <- nrow(Twitter_binDTM)
Twitter_ki <- sum(Twitter_binDTM[, Twitter_coocTerm])
Twitter_kj <- colSums(Twitter_binDTM)
names(Twitter_kj) <- colnames(Twitter_binDTM)
Twitter_kij <- Twitter_coocCounts[Twitter_coocTerm, ]

########## MI: log(Twitter_k*Twitter_kij / (Twitter_ki * Twitter_kj) ########
mutualInformationSig <- log(Twitter_k * Twitter_kij / (Twitter_ki * Twitter_kj))
mutualInformationSig <- mutualInformationSig[order(mutualInformationSig, decreasing = TRUE)]

########## DICE: 2 X&Y / X + Y ##############
dicesig <- 2 * Twitter_kij / (Twitter_ki + Twitter_kj)
dicesig <- dicesig[order(dicesig, decreasing=TRUE)]

########## Log Likelihood ###################
logsig <- 2 * ((Twitter_k * log(Twitter_k)) - (Twitter_ki * log(Twitter_ki)) - (Twitter_kj * log(Twitter_kj)) + (Twitter_kij * log(Twitter_kij)) 
               + (Twitter_k - Twitter_ki - Twitter_kj + Twitter_kij) * log(Twitter_k - Twitter_ki - Twitter_kj + Twitter_kij) 
               + (Twitter_ki - Twitter_kij) * log(Twitter_ki - Twitter_kij) + (Twitter_kj - Twitter_kij) * log(Twitter_kj - Twitter_kij) 
               - (Twitter_k - Twitter_ki) * log(Twitter_k - Twitter_ki) - (Twitter_k - Twitter_kj) * log(Twitter_k - Twitter_kj))


logsig <- logsig[order(logsig, decreasing=T)]

# Put all significance statistics in one Data-Frame
Twitter_resultOverView <- data.frame(
  names(sort(Twitter_kij, decreasing=T)[1:10]), sort(Twitter_kij, decreasing=T)[1:10],
  names(mutualInformationSig[1:10]), mutualInformationSig[1:10], 
  names(dicesig[1:10]), dicesig[1:10], 
  names(logsig[1:10]), logsig[1:10],
  row.names = NULL)

colnames(Twitter_resultOverView) <- c("Freq-terms", "Freq", "MI-terms", "MI", "Dice-Terms", "Dice", "LL-Terms", "LL")


# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
Twitter_coocTerm <- "coronavirus"

Twitter_coocs <- calculateCoocStatistics(Twitter_coocTerm, Twitter_binDTM, measure="LOGLIK")

Twitter_resultGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# The structure of the temporary graph object is equal to that of the Twitter_resultGraph
Twitter_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))

# Fill the data.frame to produce the correct number of lines
Twitter_tmpGraph[1:numberOfCoocs, 3] <- Twitter_coocs[1:numberOfCoocs]
# Entry of the search word into the first column in all lines
Twitter_tmpGraph[, 1] <- Twitter_coocTerm
# Entry of the co-occurrences into the second column of the respective line
Twitter_tmpGraph[, 2] <- names(Twitter_coocs)[1:numberOfCoocs]
# Set the significances
Twitter_tmpGraph[, 3] <- Twitter_coocs[1:numberOfCoocs]

# Attach the triples to Twitter_resultGraph
Twitter_resultGraph <- rbind(Twitter_resultGraph, Twitter_tmpGraph)

# Iteration over the most significant numberOfCoocs co-occurrences of the search term
for (i in 1:numberOfCoocs){
  
  # Calling up the co-occurrence calculation for term i from the search words co-occurrences
  newTwitter_coocTerm <- names(Twitter_coocs)[i]
  Twitter_coocs2 <- calculateCoocStatistics(newTwitter_coocTerm, Twitter_binDTM, measure="LOGLIK")
  
  #print the co-occurrences
  Twitter_coocs2[1:10]
  
  # Structure of the temporary graph object
  Twitter_tmpGraph <- data.frame(from = character(), to = character(), sig = numeric(0))
  Twitter_tmpGraph[1:numberOfCoocs, 3] <- Twitter_coocs2[1:numberOfCoocs]
  Twitter_tmpGraph[, 1] <- newTwitter_coocTerm
  Twitter_tmpGraph[, 2] <- names(Twitter_coocs2)[1:numberOfCoocs]
  Twitter_tmpGraph[, 3] <- Twitter_coocs2[1:numberOfCoocs]
  
  #Append the result to the result graph
  Twitter_resultGraph <- rbind(Twitter_resultGraph, Twitter_tmpGraph[2:length(Twitter_tmpGraph[, 1]), ])
}

require(igraph)

# Set the graph and type
Twitter_graphNetwork <- graph.data.frame(Twitter_resultGraph, directed = F)

# Identification of all nodes with less than 2 edges
Twitter_graphVs <- V(Twitter_graphNetwork)[degree(Twitter_graphNetwork) < 2]
# These edges are removed from the graph
Twitter_graphNetwork <- delete.vertices(Twitter_graphNetwork, Twitter_graphVs) 

# Assign colors to edges and nodes (searchterm blue, rest orange)
V(Twitter_graphNetwork)$color <- ifelse(V(Twitter_graphNetwork)$name == Twitter_coocTerm, 'cornflowerblue', 'orange') 

# Edges with a significance of at least 50% of the maximum sig- nificance in the graph are drawn in orange
Twitter_halfMaxSig <- max(E(Twitter_graphNetwork)$sig) * 0.5
E(Twitter_graphNetwork)$color <- ifelse(E(Twitter_graphNetwork)$sig > Twitter_halfMaxSig, "coral", "azure3")

# Disable edges with radius
E(Twitter_graphNetwork)$curved <- 0
# Size the nodes by their degree of networking
V(Twitter_graphNetwork)$size <- log(degree(Twitter_graphNetwork)) * 5

# All nodes must be assigned a standard minimum-size
V(Twitter_graphNetwork)$size[V(Twitter_graphNetwork)$size < 5] <- 4

# edge thickness
E(Twitter_graphNetwork)$width <- 1

# Finaler Plot
library(visNetwork)
Twitter_network <- toVisNetworkData(Twitter_graphNetwork)
Twitter_Network_2 <- visNetwork(nodes = Twitter_network$nodes %>% 
             mutate(font.size = if_else(id == "coronavirus",25,15)),
           
           edges = Twitter_network$edges %>% 
             mutate(color = NA), 
           # main = "Twitter keyword Co-occurrence", 
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE) 
```



# Flow 4 - STM Topic Modeling

For topic modelling we considered Latent Dirichlet Allocation (LDA) and Structural Topic Modeling (STM) [21] and we went with STM, since it incorporates metadata (information about each document) into the topic modeling framework, so that we can discover topics and estimate their relationship to document metadata. Incorporating the meta data will “affect the frequency with which a topic is discussed” as well as “the word rate use within a given topic–that is, how a particular topic is discussed” [22].
We discovered topics distributed in news articles, tweets and Reddit comments on a weekly basis over the almost three-month period and compared the topics between the three media outlets. We used metrics like residuals, the semantic coherence of the topics, the likelihood for held-out datasets to evaluate the model to select the best number of topics.

  •	Held-out likelihood estimation: the estimation of the probability of words appearing within a document when those words have been removed from the document in the estimation step [23]. It is similar to cross-validation. 
  
  •	Residuals checks: testing for overdispersion of the variance of the multinomial within the data generating process of the topic modeling [24]. If the residuals are over dispersed, it could be that more topics are needed to soak up some of the extra variance. 
  
  •	Semantic coherence: Similar to mutual information criterion, is maximized when the most probable words in a given topic frequently co-occur together [25]. This metric is similar to human judgment of topic quality [26]. However, attaining high semantic coherence was relatively easy by having a few topics dominated by very common words [27]. Therefore, we will look at both semantic coherence and exclusivity of words to topics, the tradeoff of these two parameters will indicate us a best K.


STM is an unsupervised machine learning model where we have to define the number of topic K in advance, similar to k-means clustering we do not know ahead how many topics we should use and there is no magic number of topics K for any given corpus. Therefore, we trained a group of topic models with different numbers of K (topics) and then evaluated how many topics are appropriate. Ideally, we should search K from 1 to Inf, however in this project we will limit the search from K = 2 to K = 25.
In all data sources, the held-out likelihoods were highest at K=25, and the residuals were lowest also around K=25, therefore a good number of topics would be 25. The maps between semantic coherence and exclusivity of words also showed that K=25 would be a sufficient number. We only show here the plot of Twitter data, News & Reddit plot can be found in the Appendix 1.


We then ran the structural topic modeling using `stm` package. The results of topic modeling are showed in Figure 4. The plots show the topic prevalence in each corpus (Twitter, News, Reddit), and top 8 words that have high probability belong to that topic (here we only show top 10 topics, full plots can be found in the Appendix 2). 

A closer look into top 8 topics in each source showed us some interesting pattern. 
1.	If we only look into top 8 key words in each topic, the key topics in two social media sources are quite similar. The key focuses are about something like “the spread of virus (Twitter topic 12 – Reddit topic 18”,  “negative reaction to the situation (Twitter topic 9 – Reddit topic 17)”, “update on the information (Twitter topic 15 - Reddit topic 1)”, “quarantine time (Twitter topic 17 – Reddit topic 6).

2.	However, when looking into News topics, we can see a relatively difference pattern in the focus of key topics (based on top 8 key words in each topic). In News topic are more related to “political matters – topic 22”, “situation in other country - topic 19”, “economy situation – topic 4”, “hope – topic 13”.

The difference in the focused content in key topics of our media source reflected the nature characteristic of the source (i.e. social media more reflect more fragmented in topic with high level of emotional reflection).

## Reddit data

```{r}
Reddit_stm <- convert(Reddit_DFM, to = "stm")
library(tidyverse)
library(stm)
library(furrr)
plan(multiprocess)

Reddit_many_models <- data_frame(K = c(2:25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(Reddit_DFM, K = .,
                                               verbose = FALSE)))

Reddit_heldout <- make.heldout(Reddit_DFM)

Reddit_k_result <- Reddit_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, Reddit_DFM),
         eval_heldout = map(topic_model, eval.heldout, Reddit_heldout$missing),
         residual = map(topic_model, checkResiduals, Reddit_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

Reddit_k_result[[3]]


Reddit_chart1 <- Reddit_k_result %>%
  transmute(K,
            # `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics - Reddit"
       # ,
       # subtitle = "These diagnostics indicate that a good number of topics would be"
       ) +
  scale_x_continuous(breaks = c(seq(from = 2, to = 25, by = 3)))


Reddits_chart2 <- Reddit_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(18:25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence"
       # ,
       # subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity"
       )


New_combine <- gridExtra::grid.arrange(Reddit_chart1, Reddits_chart2)

ggsave(filename = "Reddit_choosing_K.jpg", plot = New_combine, dpi = 900, scale = 1.5)


set.seed(123456)
Reddit_topic_model <- stm(
  documents = Reddit_stm$documents, 
  vocab = Reddit_stm$vocab,
  K = 25,
  prevalence =~ s(week),
  data = Reddit_stm$meta,
  init.type = "Spectral")
```

```{r}
Reddit_td_beta <- tidy(Reddit_topic_model)

Reddit_td_gamma <- tidy(Reddit_topic_model, matrix = "gamma",
                      document_names = rownames(Reddit_DFM))

library(ggthemes)

Reddit_top_terms <- Reddit_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

Reddit_gamma_terms <- Reddit_td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(Reddit_top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))

Reddit_top_terms_5 <- Reddit_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()


Reddit_gamma_terms_by_week <- Reddit_td_gamma %>%
  left_join(., docvars(Reddit_DFM) %>% mutate(document = docnames(Reddit_DFM))) %>%
  group_by(week, topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  slice(1:2) %>%
  left_join(., Reddit_top_terms_5, by = "topic") %>%
  ungroup() %>%
  mutate(topic = as.factor(paste0(topic, " - ", terms)))

Reddit_topic_by_week <- ggplot(Reddit_gamma_terms_by_week, aes(x = week, y = gamma, label = terms, color = topic, size = gamma)) + 
  geom_point(alpha = 0.8, show.legend = T) + 
  # geom_text(hjust = -0.05, nudge_y = 0.0005, size = 2,
  #           family = "Arial", angle=10) +
  scale_x_continuous(breaks = seq(from = 3, to = 16, by = 1)) + 
  geom_vline(xintercept=12, colour="red", size=1, alpha = 0.3) +
  theme(legend.position="right", 
        legend.text = element_text(colour="black", size=5)) +
  theme_bw() + 
  labs(title = "Reddit",
       y = "Topic Proportion",
       color = "Reddit Topics") + 
  theme(plot.title = element_text(size = 15,
                                  family="Arial",
                                  face = "bold")) +
  guides(size = "none")

ggsave(filename = "Reddit_topic_by_week.jpg", plot = Reddit_topic_by_week, dpi = 500, scale = 2)

Reddit_list_topic <- Reddit_gamma_terms %>%
  top_n(10, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3.5,
            family = "Arial") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.18),
                     labels = scales::percent_format()) +
  theme_tufte(base_family = "Arial", ticks = FALSE) +
  theme(plot.title = element_text(size = 15,
                                  family="Arial", 
                                  face = "bold"),
        plot.subtitle = element_text(size = 10)) +
  labs(x = NULL, y = "Topic Proportion",
       title = "Reddit"
       # ,
       # subtitle = "With the top words that contribute to each topic"
       )


ggsave(filename = "Reddit_list_topic.jpg", plot = Reddit_list_topic, dpi = 500, scale = 2)
```


## News data

```{r}
News_stm <- convert(News_DFM, to = "stm")


library(stm)
library(furrr)
plan(multiprocess)

News_many_models <- data_frame(K = c(2:25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(News_DFM, K = .,
                                               verbose = FALSE)))

News_heldout <- make.heldout(News_DFM)

News_k_result <- News_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, News_DFM),
         eval_heldout = map(topic_model, eval.heldout, News_heldout$missing),
         residual = map(topic_model, checkResiduals, News_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

News_k_result


New_chart1 <- News_k_result %>%
  transmute(K,
            # `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL
       ,
       title = "Model diagnostics by number of topics - News"
       # ,
       # subtitle = "These diagnostics indicate that a good number of topics would be around 60"
       )


New_chart2 <- News_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(18:25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity"
       ,
       title = "Comparing exclusivity and semantic coherence"
       # ,
       # subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity"
       )


New_combine <- gridExtra::grid.arrange(New_chart1, New_chart2)

ggsave(filename = "News_choosing_K.jpg", plot = New_combine, dpi = 900, scale = 1.5)


set.seed(123456)
News_topic_model <- stm(
  documents = News_stm$documents, 
  vocab = News_stm$vocab,
  K = 25,
  prevalence =~ s(week),
  data = News_stm$meta,
  init.type = "Spectral")
```

```{r}
News_td_beta <- tidy(News_topic_model)

News_td_gamma <- tidy(News_topic_model, matrix = "gamma",
                 document_names = rownames(News_DFM))

library(ggthemes)

News_top_terms <- News_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

News_gamma_terms <- News_td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(News_top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))


News_top_terms_5 <- News_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

News_gamma_terms_by_week <- News_td_gamma %>%
  left_join(., docvars(News_DFM) %>% mutate(document = docnames(News_DFM))) %>%
  group_by(week, topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  slice(1:2) %>%
  left_join(., News_top_terms_5, by = "topic") %>%
  ungroup() %>%
  mutate(topic = as.factor(paste0(topic, " - ", terms)))

News_topic_by_week <-  ggplot(News_gamma_terms_by_week, aes(x = week, y = gamma, label = terms, color = topic, size = gamma)) + 
  geom_point(alpha = 0.8, show.legend = T) + 
  # geom_text(hjust = -0.05, nudge_y = 0.0005, size = 2,
  #           family = "Arial", angle=10) +
  scale_x_continuous(breaks = seq(from = 3, to = 16, by = 1)) + 
  geom_vline(xintercept=12, colour="red", size=1, alpha = 0.5) + 
  theme(legend.position="top", 
        legend.text = element_text(colour="black", size=5)) +
  theme_bw() + 
  labs(title = "News",
       y = "Topic Proportion",
       color = "News Topics") + 
  theme(plot.title = element_text(size = 15,
                                  family="Arial",
                                  face = "bold")) +
  guides(size = "none")
  
ggsave(filename = "News_topic_by_week.jpg", plot = News_topic_by_week, dpi = 500, scale = 2)
  
library(ggthemes)

News_list_topic <- News_gamma_terms %>%
  top_n(10, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3.5,
            family = "Arial") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.18),
                     labels = scales::percent_format()) +
  theme_tufte(base_family = "Arial", ticks = FALSE) +
  theme(plot.title = element_text(size = 15,
                                  family="Arial",
                                  face = "bold"),
        plot.subtitle = element_text(size = 10)) +
  labs(x = NULL, y = "Topic Proportion",
       title = "News"
       # ,
       # subtitle = "With the top words that contribute to each topic"
       )


gridExtra::grid.arrange(News_list_topic,
                        Twitter_list_topic, 
                        Reddit_list_topic)

ggsave(filename = "News_list_topic.jpg", plot = News_list_topic, dpi = 500, scale = 2)


```


## Twitter data

```{r}

Twitter_stm <- convert(Twitter_DFM, to = "stm")

library(stm)
library(furrr)
plan(multiprocess)



Twitter_many_models <- data_frame(K = c(10:25)) %>%
  mutate(topic_model = future_map(K, ~stm::stm(Twitter_DFM, K = .,
                                               verbose = FALSE)))

Twitter_heldout <- make.heldout(Twitter_DFM)

Twitter_k_result <- Twitter_many_models %>%
  mutate(exclusivity = map(topic_model, exclusivity),
         semantic_coherence = map(topic_model, semanticCoherence, Twitter_DFM),
         eval_heldout = map(topic_model, eval.heldout, Twitter_heldout$missing),
         residual = map(topic_model, checkResiduals, Twitter_DFM),
         bound =  map_dbl(topic_model, function(x) max(x$convergence$bound)),
         lfact = map_dbl(topic_model, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(topic_model, function(x) length(x$convergence$bound)))

Twitter_k_result

Twitter_plot1 <- Twitter_k_result %>%
  transmute(K,
            # `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics"
       # ,
       # subtitle = "These diagnostics indicate that a good number of topics would be around 60"
       ) + 
  scale_x_continuous(breaks = seq(from = 10, to = 25, by = 3)) +
  theme(plot.title = element_text(size = 12,
                                  family="Arial", 
                                  face = "bold"))


Twitter_plot2 <- Twitter_k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(10,11,12,20,21,22,23,24,25)) %>%
  unnest(cols = c(exclusivity, semantic_coherence)) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  theme(legend.position="right") + 
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       caption = "Twitter data"
       # ,
       # subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity"
       ) +
  theme(plot.title = element_text(size = 12,
                                  family="Arial", 
                                  face = "bold"))


Plot_combine <- gridExtra::grid.arrange(Twitter_plot1, Twitter_plot2)

ggsave(filename = "Twitter_choosing_K.jpg", plot = Plot_combine, dpi = 500, scale = 1)

set.seed(123456)
Twitter_topic_model <- stm(
  documents = Twitter_stm$documents, 
  vocab = Twitter_stm$vocab,
  K = 25,
  prevalence =~ s(week),
  data = Twitter_stm$meta,
  init.type = "Spectral")

```

```{r}

Twitter_td_beta <- tidy(Twitter_topic_model)

Twitter_td_gamma <- tidy(Twitter_topic_model, matrix = "gamma",
                      document_names = rownames(Twitter_DFM))

library(ggthemes)

Twitter_top_terms <- Twitter_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(8, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()

Twitter_gamma_terms <- Twitter_td_gamma %>%
  group_by(topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(Twitter_top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topic = reorder(topic, gamma))


Twitter_top_terms_5 <- Twitter_td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(-beta) %>%
  select(topic, term) %>%
  summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest()


Twitter_gamma_terms_by_week <- Twitter_td_gamma %>%
  left_join(., docvars(Twitter_DFM) %>% mutate(document = docnames(Twitter_DFM))) %>%
  group_by(week, topic) %>%
  summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  slice(1:2) %>%
  left_join(., Twitter_top_terms_5, by = "topic") %>%
  ungroup() %>%
  mutate(topic = as.factor(paste0(topic, " - ", terms)))


Twitter_topic_by_week <- ggplot(Twitter_gamma_terms_by_week, aes(x = week, y = gamma, label = terms, color = topic, size = gamma)) + 
  geom_point(alpha = 0.8, show.legend = T) + 
  # geom_text(hjust = -0.05, nudge_y = 0.0005, size = 2,
  #           family = "Arial", angle=10) +
  scale_x_continuous(breaks = seq(from = 3, to = 16, by = 1)) + 
  geom_vline(xintercept=12, colour="red", size=1, alpha = 0.3) +
  # theme(legend.text = element_text(colour="black", size=5),
  #       legend.title = "Twitter Topics") +
  theme_bw() + 
  labs(title = "Twitter",
       y = "Topic Proportion",
       color = "Twitter Topics") + 
  theme(plot.title = element_text(size = 15,
                                  family="Arial",
                                  face = "bold")) +
  guides(size = "none")

ggsave(filename = "Twitter_topic_by_week.jpg", plot = Twitter_topic_by_week, dpi = 500, scale = 2)





# -------------------------------------------------------------------------



Twitter_list_topic <- Twitter_gamma_terms %>%
  top_n(10, gamma) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0005, size = 3.5,
            family = "Arial") +
  coord_flip() +
  scale_y_continuous(expand = c(0,0),
                     limits = c(0, 0.24),
                     labels = scales::percent_format()) +
  theme_tufte(base_family = "Arial", ticks = FALSE) +
  theme(plot.title = element_text(size = 15,
                                  family="Arial",
                                  face = "bold"),
        plot.subtitle = element_text(size = 10)) +
  labs(x = NULL, y = "Topic Proportion",
       title = "Twitter"
       # ,
       # subtitle = "With the top words that contribute to each topic"
       )

library(tidyverse)
ggsave(filename = "Twitter_list_topic.jpg", plot = Twitter_list_topic, dpi = 500, scale = 1.5)




# -------------------------------------------------------------------------

Topic_Keywords <- gridExtra::grid.arrange(Twitter_list_topic, 
                                          Reddit_list_topic,
                                          News_list_topic,
                                          # layout_matrix = matrix(seq_len(2 *2)),
                                          nrow = 3,
                                          ncol = 1)

ggsave(filename = "Topic_Keywords2.jpg", plot = Topic_Keywords, dpi = 900, scale = 1.8)

# -------------------------------------------------------------------------

Topic_weekly_3sources <- gridExtra::grid.arrange(Twitter_topic_by_week, 
                                          Reddit_topic_by_week,
                                          News_topic_by_week,
                                          # layout_matrix = matrix(seq_len(2 *2)),
                                          nrow = 1,
                                          ncol = 3)

ggsave(filename = "Topic_weekly_3sources.jpg", plot = Topic_weekly_3sources, dpi = 900, scale = 1.5)


# -------------------------------------------------------------------------


library(cowplot)
install.packages("ggpubr")
library(ggpubr)
grid.newpage()
ggpubr::

Topic_weekly_legend <- gridExtra::grid.arrange(as_ggplot(get_legend(Twitter_topic_by_week)), 
                                                 as_ggplot(get_legend(Reddit_topic_by_week)),
                                                 as_ggplot(get_legend(News_topic_by_week)),
                                                 # layout_matrix = matrix(seq_len(2 *2)),
                                                 nrow = 1,
                                                 ncol = 3)

ggsave(filename = "Topic_weekly_legends.jpg", plot = Topic_weekly_legend, dpi = 900, scale = 1.5)


Topic_by_week <- gridExtra::grid.arrange(Topic_weekly_3sources,
                        Topic_weekly_legend,
                        nrow = 2,
                        ncol =1)

ggsave(filename = "Topic_by_week_2.jpg", plot = Topic_by_week, dpi = 900, scale = 1.5)


```

# Flow 5 - Topic similarity

We tried to understand the topics similarity in three media sources by looking into the number of common words that a topic share with other topics. The assumption is that we only count the commons between two topics, regardless how many words each topic has in total. We follow these steps to calculate topic similarity:

  -	Extract probability of a word belonging to a topic from STM model, rank it in a descending manner and select words with the probability > 0.0005.
  
  -	Crosstab topics of two sources to get the number of common words.
  
  -	We select 5 pair of topics with highest number of common words in each pair of crosstabulation between these sources.

  -	Create a network where each connection indicates a relationship (by common words) between topics.
From network visualization, we could see that, there were some connection (by common words) in topic between three media sources. Twitter and News seemed to have relatively high number of connection (common words). However, since we selected words with relatively low probability threshold > 0.0005, the connection here was somewhat weak, therefore we only used this as a hint without further analysis.


```{r}
library(descr)
News_td_beta <- tidy(News_topic_model)

Twitter_td_beta <- tidy(Twitter_topic_model)

Reddit_td_beta <- tidy(Reddit_topic_model)


News_topic_1 <- News_td_beta %>%
  filter(topic == 1) %>%
  arrange(desc(beta)) %>%
  filter(beta > 0.005) %>%
  select(2)

Twitter_topic_1 <- Twitter_td_beta %>%
  filter(topic == 1) %>%
  select(2)


join <- inner_join(News_topic_1, Twitter_topic_1, "term")

nrow(News_topic_1)
nrow(Twitter_topic_1)
nrow(join)

News_list <- list()
Twitter_list <- list()
Reddit_list <- list()
  
for (i in c(1:25)) {
  News_list[[i]] <- News_td_beta %>%
    filter(topic == i) %>%
    arrange(desc(beta)) %>%
    filter(beta > 0.0005) %>%
    select(2)
  
  Twitter_list[[i]] <- Twitter_td_beta %>%
    filter(topic == i) %>%
    arrange(desc(beta)) %>%
    filter(beta > 0.0005) %>%
    select(2)
  
  Reddit_list[[i]] <- Reddit_td_beta %>%
    filter(topic == i) %>%
    arrange(desc(beta)) %>%
    filter(beta > 0.0005) %>%
    select(2)
}

News_vs_Twitter_2 <- News_vs_Twitter

seq <- rep(1:25, each=25)

seq2 <- rep(1:25, times = 25)

length(seq)

for (i in seq) {
  for (j in seq2) {
    for (k in seq_along(1:length(seq))) {
      News_vs_Twitter[[k]] <- inner_join(News_list[[i]], Twitter_list[[j]]) %>%
        nrow()
    }
  }
}



News_vs_Twitter <- data.frame()

for (i in (1:25)) {
  News_vs_Twitter[i,1] <- inner_join(News_list[[1]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,2] <- inner_join(News_list[[2]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,3] <- inner_join(News_list[[3]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,4] <- inner_join(News_list[[4]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,5] <- inner_join(News_list[[5]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,6] <- inner_join(News_list[[6]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,7] <- inner_join(News_list[[7]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,8] <- inner_join(News_list[[8]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,9] <- inner_join(News_list[[9]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,10] <- inner_join(News_list[[10]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,11] <- inner_join(News_list[[11]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,12] <- inner_join(News_list[[12]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,13] <- inner_join(News_list[[13]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,14] <- inner_join(News_list[[14]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,15] <- inner_join(News_list[[15]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,16] <- inner_join(News_list[[16]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,17] <- inner_join(News_list[[17]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,18] <- inner_join(News_list[[18]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,19] <- inner_join(News_list[[19]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,20] <- inner_join(News_list[[20]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,21] <- inner_join(News_list[[21]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,22] <- inner_join(News_list[[22]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,23] <- inner_join(News_list[[23]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,24] <- inner_join(News_list[[24]], Twitter_list[[i]]) %>%
    nrow()
  News_vs_Twitter[i,25] <- inner_join(News_list[[25]], Twitter_list[[i]]) %>%
    nrow()
}


df <- News_vs_Twitter %>%
  mutate(Max_row = apply(News_vs_Twitter, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(News_vs_Twitter == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}


News_vs_Twitter_Final <- data.frame(matrix(unlist(Position_2), nrow=length(Position_2), byrow=T)) %>%
  rename(News = X1, Twitter = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)


# Twitter vs News ---------------------------------------------------------

News_vs_Twitter

Twitter_vs_News <- t(News_vs_Twitter) %>%
  as.data.frame()

df <- Twitter_vs_News %>%
  mutate(Max_row = apply(Twitter_vs_News, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(Twitter_vs_News == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}

Position_2[[22]] <- Position_2[[22]][1,]

Twitter_vs_News_Final <- data.frame(matrix(unlist(Position_2), nrow=25, byrow=T)) %>%
  rename(Twitter = X1, News = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)


# News vs Reddit ----------------------------------------------------------

News_vs_Reddit <- data.frame()

for (i in (1:25)) {
  News_vs_Reddit[i,1] <- inner_join(News_list[[1]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,2] <- inner_join(News_list[[2]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,3] <- inner_join(News_list[[3]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,4] <- inner_join(News_list[[4]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,5] <- inner_join(News_list[[5]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,6] <- inner_join(News_list[[6]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,7] <- inner_join(News_list[[7]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,8] <- inner_join(News_list[[8]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,9] <- inner_join(News_list[[9]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,10] <- inner_join(News_list[[10]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,11] <- inner_join(News_list[[11]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,12] <- inner_join(News_list[[12]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,13] <- inner_join(News_list[[13]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,14] <- inner_join(News_list[[14]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,15] <- inner_join(News_list[[15]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,16] <- inner_join(News_list[[16]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,17] <- inner_join(News_list[[17]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,18] <- inner_join(News_list[[18]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,19] <- inner_join(News_list[[19]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,20] <- inner_join(News_list[[20]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,21] <- inner_join(News_list[[21]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,22] <- inner_join(News_list[[22]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,23] <- inner_join(News_list[[23]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,24] <- inner_join(News_list[[24]], Reddit_list[[i]]) %>%
    nrow()
  News_vs_Reddit[i,25] <- inner_join(News_list[[25]], Reddit_list[[i]]) %>%
    nrow()
}


df <- News_vs_Reddit %>%
  mutate(Max_row = apply(News_vs_Reddit, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(News_vs_Reddit == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}


News_vs_Reddit_Final <- data.frame(matrix(unlist(Position_2), nrow=length(Position_2), byrow=T)) %>%
  rename(News = X1, Reddit = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)


# Reddit vs News ----------------------------------------------------------
News_vs_Reddit

Reddit_vs_News <- t(News_vs_Reddit) %>%
  as.data.frame()

df <- Reddit_vs_News %>%
  mutate(Max_row = apply(Reddit_vs_News, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(Reddit_vs_News == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}


Reddit_vs_News_Final <- data.frame(matrix(unlist(Position_2), nrow=length(Position_2), byrow=T)) %>%
  rename(Reddit = X1, News = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)



# Twitter vs Reddit -------------------------------------------------------

Twitter_vs_Reddit <- data.frame()

for (i in (1:25)) {
  Twitter_vs_Reddit[i,1] <- inner_join(Twitter_list[[1]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,2] <- inner_join(Twitter_list[[2]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,3] <- inner_join(Twitter_list[[3]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,4] <- inner_join(Twitter_list[[4]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,5] <- inner_join(Twitter_list[[5]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,6] <- inner_join(Twitter_list[[6]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,7] <- inner_join(Twitter_list[[7]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,8] <- inner_join(Twitter_list[[8]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,9] <- inner_join(Twitter_list[[9]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,10] <- inner_join(Twitter_list[[10]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,11] <- inner_join(Twitter_list[[11]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,12] <- inner_join(Twitter_list[[12]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,13] <- inner_join(Twitter_list[[13]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,14] <- inner_join(Twitter_list[[14]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,15] <- inner_join(Twitter_list[[15]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,16] <- inner_join(Twitter_list[[16]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,17] <- inner_join(Twitter_list[[17]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,18] <- inner_join(Twitter_list[[18]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,19] <- inner_join(Twitter_list[[19]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,20] <- inner_join(Twitter_list[[20]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,21] <- inner_join(Twitter_list[[21]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,22] <- inner_join(Twitter_list[[22]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,23] <- inner_join(Twitter_list[[23]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,24] <- inner_join(Twitter_list[[24]], Reddit_list[[i]]) %>%
    nrow()
  Twitter_vs_Reddit[i,25] <- inner_join(Twitter_list[[25]], Reddit_list[[i]]) %>%
    nrow()
}


df <- Twitter_vs_Reddit %>%
  mutate(Max_row = apply(Twitter_vs_Reddit, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(Twitter_vs_Reddit == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}

Position_2[[13]] <- Position_2[[13]][1,]


Twitter_vs_Reddit_Final <- data.frame(matrix(unlist(Position_2), nrow=length(Position_2), byrow=T)) %>%
  rename(Twitter = X1, Reddit = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)

# Reddit vs Twitter ----------------------------------------------------------
Twitter_vs_Reddit

Reddit_vs_Twitter <- t(Twitter_vs_Reddit) %>%
  as.data.frame()

df <- Reddit_vs_Twitter %>%
  mutate(Max_row = apply(Reddit_vs_Twitter, 1, max))

a <- df %>%
  pull(Max_row) %>%
  as.vector()

Position <- list()

for (j in c(1:25)) {
  Position[[j]] <- which(Reddit_vs_Twitter == a[[j]], arr.ind=TRUE, useNames = T)
}

Position_2 <- list()

for (i in c(1:25)) {
  Position_2 [[i]] <- Position[[i]] %>%
    as.data.frame() %>%
    filter(row == i)
}


Reddit_vs_Twitter_Final <- data.frame(matrix(unlist(Position_2), nrow=length(Position_2), byrow=T)) %>%
  rename(Reddit = X1, Twitter = X2) %>%
  mutate(Number_of_similar_word = a) %>%
  arrange(desc(Number_of_similar_word)) %>%
  top_n(5)
```


```{r}
News_vs_Twitter_Final <- News_vs_Twitter_Final %>%
  mutate(News = paste0("News", News),
         Twitter = paste0("Twitter", Twitter)) %>%
  rename(from = News, 
         to = Twitter)

Twitter_vs_News_Final <- Twitter_vs_News_Final %>%
  mutate(News = paste0("News", News),
         Twitter = paste0("Twitter", Twitter)) %>%
  rename(from = Twitter, 
         to = News)

Reddit_vs_News_Final <- Reddit_vs_News_Final %>%
  mutate(News = paste0("News", News),
         Reddit = paste0("Reddit", Reddit)) %>%
  rename(from = Reddit, 
         to = News)

News_vs_Reddit_Final <- News_vs_Reddit_Final %>%
  mutate(News = paste0("News", News),
         Reddit = paste0("Reddit", Reddit)) %>%
  rename(from = News, 
         to = Reddit)

Twitter_vs_Reddit_Final <- Twitter_vs_Reddit_Final %>%
  mutate(Twitter = paste0("Twitter", Twitter),
         Reddit = paste0("Reddit", Reddit)) %>%
  rename(from = Twitter, 
         to = Reddit)


Reddit_vs_Twitter_Final <- Reddit_vs_Twitter_Final %>%
  mutate(Twitter = paste0("Twitter", Twitter),
         Reddit = paste0("Reddit", Reddit)) %>%
  rename(from = Reddit, 
         to = Twitter)

edges <- bind_rows(News_vs_Twitter_Final,
                   Twitter_vs_News_Final,
                   Reddit_vs_News_Final, 
                   News_vs_Reddit_Final,
                   Twitter_vs_Reddit_Final,
                   Reddit_vs_Twitter_Final) %>%
  rename(Num_words = Number_of_similar_word)


nodes <- edges %>%
  bind_rows(select(., from = to)) %>%
  select(id = from) %>%
  distinct() %>%
  mutate(label = id) %>%
  mutate(., group = if_else(str_detect(nodes$id, "News"), "News",
                            if_else(str_detect(nodes$id, "Twitter"), "Twitter", 
                                    "Reddit")))

visNetwork(nodes = nodes, edges = edges,
           height = "900px", 
           width = "900px") %>%
  visPhysics(solver = "forceAtlas2Based", forceAtlas2Based = list(gravitationalConstant = -20)) %>%
  visInteraction(dragNodes = TRUE, 
                 dragView = T, 
                 zoomView = TRUE,
                 hideEdgesOnDrag = TRUE) %>%
  visSave(file = "Topic_Similairty_network.html")
```


# Flow 6 - Topic Model Sentiments

To investigate sentiments in news articles, reddit comments and tweets, we used tokens extracted from the documents. We calculated sentiment scores with the AFINN lexicon developed by Finn Årup Nielsen, which is included in the tidytext package. The AFINN lexicon is a list of English terms manually rated for valence with an integer between -5 (negative) and +5 (positive). It comprises 878 positive terms and 1,598 negative terms. The sentiment score is the percentage of the difference between the absolute sum of positive terms and negative terms divided by the total sum of them.

With the purpose of examining from several perspectives, we also made use of the NRC lexicon from Saif Mohammad and Peter Turney. The NRC lexicon categorizes words in a binary fashion (“yes”/ “no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust. We filtered out the positive and negative categories which left us with 4,463 distinct words in the lexicon (Note: one word can be in multiple sentiment categories). We counted the number of words in each category and computed their percentage distribution in each of our sources.

On the topic-based level, we assigned the sentiment value for each topic word based on the lexicon and the probability of a word in a topic given by the STM topic model as the term weight of the word. As a result, topic sentiment score is calculated as
$$Sentiment\ Score\ of\ Topic\ A = \sum_{i=1}^{n} (prob(word_i|Topic_A)*sentiment\ value(word_i))$$

An overall topic sentiment score is computed by multiplying the sentiment value by the probability of words and summing the products in a chosen topic. Multiple topics could have the same words with different probability values. Therefore, the sentiment score of a topic would be distinguished from others even if the same set of words appear in different topics. 


## Reddit Data

### Make a data frame with the AFINN dictionary

```{r}
afinn <- get_sentiments("afinn")

Reddit_ext <- convert(Reddit_DFM, to = "tripletlist")
Reddit_df <- data.frame(doc = Reddit_ext$document,
                         word = Reddit_ext$feature,
                         freq = Reddit_ext$frequency)
Reddit_df$word <- as.character(Reddit_df$word)

Reddit_afinn <- Reddit_df %>% 
  inner_join(afinn, by = "word") %>% 
  mutate(score = freq * value) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>% 
  group_by(week, sentiment) %>% 
  summarise(sentiment_score = sum(score)) %>% 
  ungroup() %>% 
  mutate(source = "Reddit")
```

### Create a data frame to plot sentiment

```{r}
Reddit_afinn_plot <- Reddit_afinn %>% 
  pivot_wider(names_from = sentiment, values_from = sentiment_score) %>% 
  mutate(sum = positive + (-1)*negative, 
         pos_percent = positive/sum*100,
         neg_percent = (-1)*negative/sum*100, 
         polarity = round((pos_percent - neg_percent),3)) %>%
  select(week, polarity, source)
```

### Extract beta matrix to data frame format. Using AFINN lexicon, make a data frame with the sentiment for each word

```{r}
afinn <- get_sentiments("afinn")
Reddit_beta <- tidy(Reddit_topic_model, matrix = "beta")
Reddit_topic_sc <- inner_join(Reddit_beta, afinn, by = c("term" = "word")) %>% 
  mutate(score = beta * value) %>% 
  group_by(topic) %>% 
  summarise(sentiment_score = sum(score))
Reddit_topic_sc$topic <- factor(Reddit_topic_sc$topic, levels = c(1:25))
```


## Twitter Data

### Make a data frame with the AFINN dictionary

```{r}
afinn <- get_sentiments("afinn")

Twitter_ext <- convert(Twitter_DFM, to = "tripletlist")
Twitter_df <- data.frame(doc = Twitter_ext$document,
                      word = Twitter_ext$feature,
                      freq = Twitter_ext$frequency)
Twitter_df$word <- as.character(Twitter_df$word)

Twitter_afinn <- Twitter_df %>% 
  inner_join(afinn, by = "word") %>% 
  mutate(score = freq * value) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>% 
  group_by(week, sentiment) %>% 
  summarise(sentiment_score = sum(score)) %>% 
  ungroup() %>% 
  mutate(source = "Twitter")
```

### Create a data frame to plot sentiment

```{r}
Twitter_afinn_plot <- Twitter_afinn %>% 
  pivot_wider(names_from = sentiment, values_from = sentiment_score) %>% 
  mutate(sum = positive + (-1)*negative, 
         pos_percent = positive/sum*100,
         neg_percent = (-1)*negative/sum*100, 
         polarity = round((pos_percent - neg_percent),3)) %>%
  select(week, polarity, source)
```

### Extract beta matrix to data frame format. Using AFINN lexicon, make a data frame with the sentiment for each word

```{r}
Twitter_beta <- tidy(Twitter_topic_model, matrix = "beta")
Twitter_topic_sc <- inner_join(Twitter_beta, afinn, by = c("term" = "word")) %>%  
  mutate(score = beta * value) %>% 
  group_by(topic) %>% 
  summarise(sentiment_score = sum(score))
Twitter_topic_sc$topic <- factor(Twitter_topic_sc$topic, levels = c(1:25))
```


## News Data

### Make a data frame with the AFINN dictionary

```{r}
afinn <- get_sentiments("afinn")

News_ext <- convert(News_DFM, to = "tripletlist")
News_df <- data.frame(doc = News_ext$document,
                        word = News_ext$feature,
                        freq = News_ext$frequency)
News_df$word <- as.character(News_df$word)

News_afinn <- News_df %>% 
  inner_join(afinn, by = "word") %>% 
  mutate(score = freq * value) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  mutate(sentiment = ifelse(score > 0, "positive", "negative")) %>% 
  group_by(week, sentiment) %>% 
  summarise(sentiment_score = sum(score)) %>% 
  ungroup() %>% 
  mutate(source = "News")
```

### Create a data frame to plot sentiment

```{r}
News_afinn_plot <- News_afinn %>% 
  pivot_wider(names_from = sentiment, values_from = sentiment_score) %>% 
  mutate(sum = positive + (-1)*negative, 
         pos_percent = positive/sum*100,
         neg_percent = (-1)*negative/sum*100, 
         polarity = round((pos_percent - neg_percent),3)) %>%
  select(week, polarity, source)
```

### Extract beta matrix to data frame format. Using AFINN lexicon, make a data frame with the sentiment for each word

```{r}
News_beta <- tidy(News_topic_model, matrix = "beta")
News_topic_sc <- inner_join(News_beta, afinn, by = c("term" = "word")) %>% 
  mutate(score = beta * value) %>% 
  group_by(topic) %>% 
  summarise(sentiment_score = sum(score))
News_topic_sc$topic <- factor(News_topic_sc$topic, levels = c(1:25))
```


## Plot for Sentiment Analysis

### Sentiment changes in all three sources - AFINN

We examined the sentiment in news, Reddit and Twitter about coronavirus from week 3 to week 16 of 2020.

```{r}
afinn_chart_all <- rbind(Reddit_afinn_plot, News_afinn_plot, Twitter_afinn_plot)
afinn_chart_all$week <- as.numeric(as.character(afinn_chart_all$week))
afinn_chart_all %>% 
  arrange(week) %>% 
  ggplot(aes(x = week, y = polarity, color = source)) + 
  geom_point() +
  geom_path() +
  geom_hline(yintercept = 0, color = "gold3") +
  geom_vline(xintercept = 12, color = "coral2") +
  labs(x = "Week", y = "Sentiment Score") + 
  scale_x_continuous(  # This handles replacement of row 
    breaks = afinn_chart_all$week, # notice need to reuse data frame
    labels = afinn_chart_all$week) +
  scale_y_continuous(n.breaks = 10) +
  scale_colour_manual(values = c("#33A02B", "#E3211C", "#1F78B4")) +
  theme(panel.grid = element_blank(),
        panel.background = NULL,
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 16),
        legend.title = element_text(size = 16),
        legend.text = element_text(size = 16),
        legend.position = "bottom")

```

Overall, the opinion towards COVID-19 of all three sources remained negative throughout the period. Reddit was the most negative source, while news was a bit more positive than Twitter. There was a similar pattern between news articles and Twitter, as the sentiment scores were highly negative in week 3 but showed a positive trend in the subsequent weeks. At week 12 when the situation got worse in the US, the sentiment score of news articles went down then picked up again at week 15, whereas Twitter showed a consistent sentiment. For Reddit, the sentiment of comments did not have much changes, as it wavered at around -30.

### Compare different categorical sentiment - NRC

Applying NRC lexicon to inspect the data in different categorical sentiments, we presented the result in a radar chart

```{r}
nrc <- get_sentiments("nrc") %>% 
  filter(!sentiment %in%  c("positive", "negative"))

# News join and calculate score
News_nrc <- News_df %>% 
  inner_join(nrc) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  group_by(week, sentiment) %>% 
  summarise(count = sum(freq))
News_nrc_sum <- News_nrc %>% 
  group_by(sentiment) %>% 
  summarise(total_count = sum(count)) %>% 
  ungroup()
News_nrc_sum <- News_nrc_sum %>% 
  mutate(percent = total_count/ sum(News_nrc_sum$total_count),
         source = "News")

# Reddit join and calculate score
Reddit_nrc <- Reddit_df %>% 
  inner_join(nrc) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  group_by(week, sentiment) %>% 
  summarise(count = sum(freq))
Reddit_nrc_sum <- Reddit_nrc %>% 
  group_by(sentiment) %>% 
  summarise(total_count = sum(count)) %>% 
  ungroup()
Reddit_nrc_sum <- Reddit_nrc_sum %>% 
  mutate(percent = total_count/ sum(Reddit_nrc_sum$total_count),
         source = "Reddit")

# Twitter join and calculate score
Twitter_nrc <- Twitter_df %>% 
  inner_join(nrc) %>% 
  separate(doc, into = c("week", "no"), sep = "\\.") %>% 
  group_by(week, sentiment) %>% 
  summarise(count = sum(freq))
Twitter_nrc_sum <- Twitter_nrc %>% 
  group_by(sentiment) %>% 
  summarise(total_count = sum(count)) %>% 
  ungroup()
Twitter_nrc_sum <- Twitter_nrc_sum %>% 
  mutate(percent = total_count/ sum(Twitter_nrc_sum$total_count),
         source = "Twitter")

# Combine data and plot radar chart
nrc_chart_all <- rbind(Reddit_nrc_sum, News_nrc_sum, Twitter_nrc_sum)
nrc_chart_all %>% 
  select(-total_count) %>% 
  pivot_wider(names_from = source, values_from = percent) %>% 
  chartJSRadar(width = 8,
               height = 5,
               showToolTipLabel = FALSE,
               labelSize = 20)
```

In consideration of all data available, news articles showed the most positivity by coming on top of 2 categories “joy” and “trust”, but it also had the most “anger” words. Reddit were dominant in most negative categories, such as “sadness”, “fear” and “anticipation”. Most of Twitter’s posts just expressed “surprise” in their own terms. This turned out to be a relevant interpretation of the outcome of the AFINN lexicon analysis above. 

### Topic-based sentiment scores

Using STM model with the same number of topics of 25 for all three sources, we calculated the sentiment score of each topic. Again, the general opinions in most topics were negative. The sentiments were more significant in Reddit and Twitter, while in news articles most topics had the scores closer to neutral.

```{r}
# News

News_topic_sc_plot <- News_topic_sc %>% 
  ggplot(aes(x = topic, y = sentiment_score)) + 
  geom_col(fill = "#33A02B") +
  geom_hline(yintercept = 0, color = "gold3") +
  labs(x = NULL, y = NULL, subtitle = "News") + 
  scale_y_continuous(limits = c(-1, 0.2), n.breaks = 8) +
  theme(panel.grid = element_blank(),
        panel.background = NULL,
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        plot.subtitle = element_text(size = rel(1.5)))

# Reddit

Reddit_topic_sc_plot <- Reddit_topic_sc %>% 
  ggplot(aes(x = topic, y = sentiment_score)) + 
  geom_col(fill = "#E3211C") +
  geom_hline(yintercept = 0, color = "gold3") +
  labs(x = NULL, y = NULL, subtitle = "Reddit") + 
  scale_y_continuous(limits = c(-1, 0.2), n.breaks = 8) +
  theme(panel.grid = element_blank(),
        panel.background = NULL,
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        plot.subtitle = element_text(size = rel(1.5)))

# Twitter

Twitter_topic_sc_plot <- Twitter_topic_sc %>% 
  ggplot(aes(x = topic, y = sentiment_score)) + 
  geom_col(fill = "#1F78B4") +
  geom_hline(yintercept = 0, color = "gold3") +
  labs(x = "Topic", y = NULL, subtitle = "Twitter") + 
  scale_y_continuous(limits = c(-1, 0.2), n.breaks = 8) +
  theme(panel.grid = element_blank(),
        panel.background = NULL,
        axis.title = element_text(size = 16),
        axis.text = element_text(size = 14),
        plot.subtitle = element_text(size = rel(1.5)))

grid.arrange(News_topic_sc_plot, Reddit_topic_sc_plot, Twitter_topic_sc_plot)
```

We noticed some extremely negative topics in social medias. Taking a closer look, these are the topics that mostly consist of sentimental words, as in Topic 17 of Reddit (yeah, lol, holy, jesus…) and Topic 9 of Twitter (hell, yall, man, holy…). This was one downside of unsupervised topic modelling, as it grouped these sentimental words together in one topic, but the interpretation was not useful for topic discovery. Excluding the topics above, Reddit had the most negative sentiments, as in Topic 11 (virus, disease, wuhan, china…), Topic 22 (nba, drop, play, season…) and Topic 4 (case, confirm, county, state…), which showed that Reddit users were worried about the role of China in the pandemic, the rising number of confirmed cases in the states and surprisingly, the cancellation of NBA season. At the same time, news and Twitter had a few notable negativities, with topic 22 (political, pence, tweet, blame, party, attack…) and 23 (police, prison, court, law, jail…) in news and topic 2 (trump, lie, hoax, administration…) and 13 (die, flu, trump, li, whistleblower…) in Twitter. Twitter users seemed to have the concerns centered around President Trump’s behaviors, while the news media gravitated towards the conflict of political parties and worries about social security.

Among the positive topics, there was a homogeneous them in all sources. Topic 13 in the news (love, church, moment, wife, mother…), Topic 9 in Reddit (live, family, hope, friend, mom…) and Topic 14 in Twitter (family, friend, pray, thought, recovery…) had the same theme of faith and belief, though the positive scores were mild.



